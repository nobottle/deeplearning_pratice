{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNie/JARq0P93It222u98rE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobottle/deeplearning_pratice/blob/main/%EB%AA%A8%EB%A9%98%ED%85%80%2CAdaGrad%2CAdam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모멘텀\n",
        "\n",
        "v=av-(러닝레이트)*w에대한 손실함수의 기울기\n",
        "w=w+v\n",
        "\n",
        "v는 물체의 속도\n",
        "\n",
        "a는 momentum"
      ],
      "metadata": {
        "id": "TWQN7NzOqwPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class momentum:\n",
        "  def __init__(self,lr=0.01,momentun=0.9):\n",
        "    self.lr=lr\n",
        "    self.momentum=momentum\n",
        "    self.v=None #초기화때는 아무값도 담지 않는다\n",
        "\n",
        "  def update(self,params,grads):\n",
        "    if self.v is None:\n",
        "      self.v={}\n",
        "      for key,val in params.items():\n",
        "        self.v[key]=np.zeros_like(val)\n",
        "\n",
        "      for key in params.keys():\n",
        "        self.v[key]=self.momentum*self.v[key]-self.lr*grads[key]\n",
        "        params[key]+=self.v[key]\n"
      ],
      "metadata": {
        "id": "3oIAg7VXq-2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaGead\n",
        "\n",
        "신경망 학습에서는 학습률이 중요하다 값이 너무 작으면,학습시간이 길어지고,반대로 너무 크면 발산하면서 학습이 제대로 이뤄지지 않음\n",
        "\n",
        "이 학습률 정하는 효과적 기술로 학습률감소가 있다. 이는 학습을 진행하면서 학습률을 점차 줄여가는 방법\n",
        "\n",
        "처음에는 크게 학습,조금씩 작게 학습한다는 얘기\n",
        "\n",
        "학습률을 서서히 낮추는 간단한 방법은 매개변수'전체'의 학습률 값을 일괄적으로 낮추는 것\n",
        "\n",
        "AdaGead는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행\n",
        "\n",
        "h=h+(기울기)**2\n",
        "\n",
        "w=w-(학습률)*1/h**0.5*기울기\n",
        "\n",
        "매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아진다는 뜻인데,다시 말해 학습률 감소가 매개변수의 원소마다 다르게 적용됨"
      ],
      "metadata": {
        "id": "J2hkbq7QtR1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "  def __init__(self,lr=0.01):\n",
        "    self.lr=lr\n",
        "    self.h=None\n",
        "\n",
        "  def update(self,params,grads):\n",
        "    if self.h is None:\n",
        "      self.h={}\n",
        "      for key,val in params.items():\n",
        "        self.h[key]=np.zeros.like(val)\n",
        "    for key in params.keys():\n",
        "      self.h[key]+=grads[key]*grads[key]\n",
        "      params[key]-=self.lr*grads[key]/(np.sqrt(self.h[key])+1e-7)#1e-7이거는 작은 값을 더해준다는 거 즉 self.h[key]에 0이 담겨 있다 해도 0으로 나누는 사태를 막아줌"
      ],
      "metadata": {
        "id": "lR7uW1MCv8JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam\n",
        "\n",
        "모멘텀과 AdaGrad를 합한거라고 보면 됨"
      ],
      "metadata": {
        "id": "kKoqHDUGxE3q"
      }
    }
  ]
}