{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO86YhjEvd3oTAA906cn4cl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobottle/deeplearning_pratice/blob/main/04_%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5_%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "손실함수의 변수\n",
        "\n",
        "가중치와 편향이 손실함수의 변수,입력데이터는 손실함수의 계수\n",
        "\n",
        "손실함수의 최소를 찾는과정\n",
        "\n",
        "\n",
        "경사하강법\n",
        "\n",
        "다변수 함수 점x에서 가장 감소하는 방향은 gradient의 반개방향 -붙인거이다\n",
        "\n",
        "경사하강법은 gradient의 반대방향으로 한 발자국씩 내딛으며 함수값을 낮추는 방법\n",
        "\n",
        "경사하강법의 문제점\n",
        "\n",
        "출발하는 지점에 따라 결괏값이 민감하게 바뀜\n",
        "\n",
        "결국 최소점이 아니라 극소점 또는 안장점에 안착할 수도 있는 문제점이 있다\n",
        "\n",
        "초기위치에 따라 결과가 달라진다.\n",
        "\n",
        " stochastic gradient destent\n",
        "\n",
        " 데이터한장한장 함수를 구하고 총 장수의 함수들의 평균을 구함\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "rJLeaA2fgsK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from gradient_2d import numerical_gradient\n",
        "\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100): #f함수 init초기위치 lr러닝레이트 step_num몇걸음 내려갈거냐\n",
        "    x = init_x#출발위치\n",
        "    x_history = []#내려간 발자국 과거의 기록들을 모아놓은 리스트를 만들겟다\n",
        "\n",
        "    for i in range(step_num):\n",
        "        x_history.append( x.copy() )#copy원소를 바꾸면 연동된거 바뀜 즉 연동되지 않게 하고 싶으면 이거 쓰면 됨 이거를 없애버리면 도착점의 점만 기록이 된다  x_history=[x0,x1...]\n",
        "\n",
        "        grad = numerical_gradient(f, x)#함수를 이용해서 출발위치에서 그래디언트를 구한다\n",
        "        x -= lr * grad#x에서 러닝레이트를 곱한그레디언트를 빼서 x에 넣는다 즉 경사하강법\n",
        "\n",
        "    return x, np.array(x_history)#x는 100번해서 내려간 지점,그리고 과거의 기록들을 배열로 \n",
        "\n",
        "\n",
        "def function_2(x):#f1(x,y)=x**2+y**2\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])    #새로운 출발지\n",
        "\n",
        "lr = 0.1\n",
        "step_num = 20 #20걸음만 내려가겟다\n",
        "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num) #lr왼쪽은 러닝레이트 lr오른쪽은0.1\n",
        "\n",
        "plt.plot( [-5, 5], [0,0], '--b')\n",
        "plt.plot( [0,0], [-5, 5], '--b')\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
        "\n",
        "plt.xlim(-3.5, 3.5)\n",
        "plt.ylim(-4.5, 4.5)\n",
        "plt.xlabel(\"X0\")\n",
        "plt.ylabel(\"X1\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rx8SmayuASFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(2,3) # 정규분포로 초기화 2,3행렬로 만들어라\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.W)\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        z = self.predict(x)\n",
        "        y = softmax(z)\n",
        "        loss = cross_entropy_error(y, t) #정답과 실험한 데이터의 차이를 엔트로피로 계산해서 LOSS에 넣어둠\n",
        "\n",
        "        return loss\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "t = np.array([0, 0, 1])\n",
        "\n",
        "net = simpleNet()\n",
        "\n",
        "f = lambda w: net.loss(x, t)\n",
        "dW = numerical_gradient(f, net.W)\n",
        "\n",
        "print(dW)\n"
      ],
      "metadata": {
        "id": "yFUl-d8kh5R3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}